{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "156cad16",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Data science is a hot topic in today's world because it has helped companies and individuals extract value from data in unprecedented ways, particularly through ML.\n",
    "\n",
    "In today's session, we're going to be going over some of the applications of data science and ML in predicting the housing market. More specifically, we'll be talking about supervised learning. \n",
    "\n",
    "### Our story for today:\n",
    "Your cousin has made millions of dollars speculating in the housing market. He's offered to become business partners with you because of your interest in data science. He'll supply the money, and you'll supply models that predict how much various houses are worth.\n",
    "\n",
    "You ask your cousin how he's predicted real estate values in the past, and he says it is just intuition. But more questioning reveals that he's identified price patterns from houses he has seen in the past, and he uses those patterns to make predictions for new houses he is considering.\n",
    "\n",
    "At its core, machine learning works the same way. We'll give our model a labelled data set, and train it to find patterns in the data to make predictiosn for us. \n",
    "\n",
    "To begin this session, we'll start with a model called the Decision Tree. There are fancier models that give more accurate predictions. But decision trees are easy to understand, and they are the basic building block for some of the best models in data science.\n",
    "\n",
    "**NB: this narrative^ was sourced from Kaggle's Intro to Machine Learning series**\n",
    "\n",
    "## Decision Trees\n",
    "A decision tree follows a set of if-else conditions to visualize the data and classify it according to the conditions. In the example shown below, the predicted price for any house under consideration is the historical average price of houses in the same category."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "551ace1e",
   "metadata": {},
   "source": [
    "<img src = 'http://i.imgur.com/7tsb5b1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185e2920",
   "metadata": {},
   "source": [
    "#### For nerds\n",
    "To figure out which criteria are best to use, the decision tree algorithm seeks to minimize **impurity**. A popular impurity measure is the **Gini index**, calculated using the formula below:\n",
    "\n",
    "<img src = \"https://cdn-images-1.medium.com/max/300/0*pE3uG1i28u5ClQVQ.png\">\n",
    "\n",
    "where $p_i$ is the sum of squared probabilities of each class $i$. In this case $i = 2$ because we split our data into two classes: houses with more than 2 bedrooms, and houses with 2 or fewer bedrooms. \n",
    "\n",
    "Therefore, we can compute the \n",
    "\n",
    "### A better tree\n",
    "We can capture more information and produce better predictions by introducing \"splits\" in our tree. With a split, we're able to use more features to make our pricing decision. This tree is \"deeper\" than our previous one because it has more splits.\n",
    "\n",
    "We use the Gini score to determine which feature sits at the top of the tree and so on. \n",
    "\n",
    "<img src = \"http://i.imgur.com/R3ywQsR.png\">\n",
    "\n",
    "We predict the price of any house by tracing through the decision tree, always picking the path corresponding to that house's characteristics. The predicted price for the house is at the bottom of the tree. The point at the bottom where we make a prediction is called a **leaf**.\n",
    "\n",
    "## Pandas\n",
    "Before we get to modelling though, we should learn how to examine our data. We'll use the Pandas library for this. Pandas is the primary tool data scientists/consultants use for exploring and manipulating data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0a016a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1fd0297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data \n",
    "# our data is already split into training and testing\n",
    "# which we will discuss in a bit\n",
    "train = pd.read_csv('train.csv', index_col='Id')\n",
    "\n",
    "# data cleaning - dw abt this\n",
    "# Remove rows with missing target information, \n",
    "# separate target from predictors\n",
    "train.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "\n",
    "# split into feature-space and target-space\n",
    "y = train['SalePrice']\n",
    "X = train.drop(['SalePrice'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bddc5d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# we're going to split the data so we can test our model accuracy later on unseen data\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c3bcddc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1168.000000</td>\n",
       "      <td>951.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1162.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "      <td>1168.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>56.849315</td>\n",
       "      <td>70.343849</td>\n",
       "      <td>10689.642123</td>\n",
       "      <td>6.121575</td>\n",
       "      <td>5.584760</td>\n",
       "      <td>1970.965753</td>\n",
       "      <td>1984.897260</td>\n",
       "      <td>103.771945</td>\n",
       "      <td>446.023973</td>\n",
       "      <td>45.152397</td>\n",
       "      <td>...</td>\n",
       "      <td>476.273973</td>\n",
       "      <td>95.946918</td>\n",
       "      <td>49.578767</td>\n",
       "      <td>21.839041</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>15.407534</td>\n",
       "      <td>2.955479</td>\n",
       "      <td>51.267123</td>\n",
       "      <td>6.356164</td>\n",
       "      <td>2007.818493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>42.531862</td>\n",
       "      <td>24.897021</td>\n",
       "      <td>10759.366198</td>\n",
       "      <td>1.367619</td>\n",
       "      <td>1.116062</td>\n",
       "      <td>30.675495</td>\n",
       "      <td>20.733955</td>\n",
       "      <td>173.032238</td>\n",
       "      <td>459.070977</td>\n",
       "      <td>158.217499</td>\n",
       "      <td>...</td>\n",
       "      <td>211.095373</td>\n",
       "      <td>129.685939</td>\n",
       "      <td>69.433580</td>\n",
       "      <td>62.083227</td>\n",
       "      <td>31.519664</td>\n",
       "      <td>55.881148</td>\n",
       "      <td>41.648504</td>\n",
       "      <td>553.039684</td>\n",
       "      <td>2.670707</td>\n",
       "      <td>1.322639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7587.250000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1953.000000</td>\n",
       "      <td>1966.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>9600.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1972.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>384.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>482.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11700.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2001.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>721.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1378.000000</td>\n",
       "      <td>5644.000000</td>\n",
       "      <td>1127.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1418.000000</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>15500.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MSSubClass  LotFrontage        LotArea  OverallQual  OverallCond  \\\n",
       "count  1168.000000   951.000000    1168.000000  1168.000000  1168.000000   \n",
       "mean     56.849315    70.343849   10689.642123     6.121575     5.584760   \n",
       "std      42.531862    24.897021   10759.366198     1.367619     1.116062   \n",
       "min      20.000000    21.000000    1300.000000     1.000000     1.000000   \n",
       "25%      20.000000    59.000000    7587.250000     5.000000     5.000000   \n",
       "50%      50.000000    70.000000    9600.000000     6.000000     5.000000   \n",
       "75%      70.000000    80.000000   11700.000000     7.000000     6.000000   \n",
       "max     190.000000   313.000000  215245.000000    10.000000     9.000000   \n",
       "\n",
       "         YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1   BsmtFinSF2  ...  \\\n",
       "count  1168.000000   1168.000000  1162.000000  1168.000000  1168.000000  ...   \n",
       "mean   1970.965753   1984.897260   103.771945   446.023973    45.152397  ...   \n",
       "std      30.675495     20.733955   173.032238   459.070977   158.217499  ...   \n",
       "min    1872.000000   1950.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%    1953.000000   1966.000000     0.000000     0.000000     0.000000  ...   \n",
       "50%    1972.000000   1994.000000     0.000000   384.500000     0.000000  ...   \n",
       "75%    2001.000000   2004.000000   166.000000   721.000000     0.000000  ...   \n",
       "max    2010.000000   2010.000000  1378.000000  5644.000000  1127.000000  ...   \n",
       "\n",
       "        GarageArea   WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  \\\n",
       "count  1168.000000  1168.000000  1168.000000    1168.000000  1168.000000   \n",
       "mean    476.273973    95.946918    49.578767      21.839041     3.812500   \n",
       "std     211.095373   129.685939    69.433580      62.083227    31.519664   \n",
       "min       0.000000     0.000000     0.000000       0.000000     0.000000   \n",
       "25%     341.000000     0.000000     0.000000       0.000000     0.000000   \n",
       "50%     482.000000     0.000000    27.000000       0.000000     0.000000   \n",
       "75%     576.000000   168.000000    74.000000       0.000000     0.000000   \n",
       "max    1418.000000   857.000000   547.000000     552.000000   508.000000   \n",
       "\n",
       "       ScreenPorch     PoolArea       MiscVal       MoSold       YrSold  \n",
       "count  1168.000000  1168.000000   1168.000000  1168.000000  1168.000000  \n",
       "mean     15.407534     2.955479     51.267123     6.356164  2007.818493  \n",
       "std      55.881148    41.648504    553.039684     2.670707     1.322639  \n",
       "min       0.000000     0.000000      0.000000     1.000000  2006.000000  \n",
       "25%       0.000000     0.000000      0.000000     5.000000  2007.000000  \n",
       "50%       0.000000     0.000000      0.000000     6.000000  2008.000000  \n",
       "75%       0.000000     0.000000      0.000000     8.000000  2009.000000  \n",
       "max     480.000000   738.000000  15500.000000    12.000000  2010.000000  \n",
       "\n",
       "[8 rows x 36 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5a705",
   "metadata": {},
   "source": [
    "### Interpreting Description\n",
    "**Describe will only show us a description of our numeric variables**\n",
    "- count: shows number of rows with non-missing data\n",
    "- mean: average\n",
    "- std: standard deviation\n",
    "- min, 25%, 50%, 75%, max: smallest value, the number at 25% is the number that is bigger than 25% of the values for that column of information, and so on so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2111ed92",
   "metadata": {},
   "source": [
    "## Building your first ML model\n",
    "There are a total of 79 features that we can use to estimate house prices. However, we will only be using a subset of these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3e609ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train.columns) # number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "71649a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street', 'Alley',\n",
       "       'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n",
       "       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n",
       "       'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle',\n",
       "       'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea',\n",
       "       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n",
       "       'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2',\n",
       "       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC',\n",
       "       'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
       "       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
       "       'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n",
       "       'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageYrBlt',\n",
       "       'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond',\n",
       "       'PavedDrive', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n",
       "       'ScreenPorch', 'PoolArea', 'PoolQC', 'Fence', 'MiscFeature', 'MiscVal',\n",
       "       'MoSold', 'YrSold', 'SaleType', 'SaleCondition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0133acfb",
   "metadata": {},
   "source": [
    "### Choosing features\n",
    "First, for simplicity, in this workshop, we'll only work with numeric data. Before we get to modelling, our goal is to find the \"best\" features in our data set to make our predictions with. In other words, we want to isolate the features with the most predictive power and eliminate the ones that are \"noisy.\"\n",
    "\n",
    "Methods for selecting features:\n",
    "1. Intuition/domain expertise\n",
    "2. Statistical methods\n",
    "3. Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c0251aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 36 variables. Let's try to pare this down to a smaller subset!\n"
     ]
    }
   ],
   "source": [
    "# get the numeric columns\n",
    "numeric_cols = [col for col in X_train.columns if X_train[col].dtype == 'int64' or \n",
    "                X_train[col].dtype == 'float64']\n",
    "\n",
    "\n",
    "# show number of numeric columns of data\n",
    "print(f\"We have {len(numeric_cols)} variables. Let's try to pare this down to a smaller subset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b1252e",
   "metadata": {},
   "source": [
    "#### 1. Applying intuition/domain expertise\n",
    "Utilising domain knowledge is probably the fastest way to identify variables, but it may also introduce bias. Perhaps what you know from the past is not representative of what shapes the housing market today! Nonetheless, it's a technique that can expedite analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "39e4703b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the columns we're choosing from:\n",
      "\n",
      "['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']\n"
     ]
    }
   ],
   "source": [
    "print(f'These are the columns we\\'re choosing from:\\n\\n{numeric_cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fdcfb2",
   "metadata": {},
   "source": [
    "Suppose that we're consultants, and in our database & from past experience, we've identified the following variables as the most important\n",
    "- LotArea: the lot area\n",
    "- OverallQual: the quality rating of the property \n",
    "- GrLivArea: the gross living area of the property\n",
    "- GarageArea: the area of the garage of the property\n",
    "- MoSold: the month the property was sold \n",
    "- YrSold: the year the property was sold \n",
    "- TotRmsAbvGrd: the total number of rooms the property has (above ground)\n",
    "\n",
    "We can now use these variables to build our decision tree model! In this process, we're not only going to select the relevant features outlined in the bulleted list, but we're also going to standardise the features. \n",
    "\n",
    "Standardisation is done by performing the following computation: for every vector $\\overrightarrow x$ in our matrix of predictors $X$, compute $\\frac{\\overrightarrow x - \\mu_x}{\\sigma_x}$\n",
    "\n",
    "Standardisation is important because it gives equal weights/importance to each feature so that no single feature steers model performance in one direction just because it is generally bigger than the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "24a65eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess1(X):\n",
    "    \"\"\"\n",
    "    Preprocess our data by selecting the features\n",
    "    outlined above and then standardising the feature-space\n",
    "    \n",
    "    Returns: new, standardised feature-space and our original\n",
    "    target vector\n",
    "    \"\"\"\n",
    "    X1 = X.copy()\n",
    "    vars1 = ['LotArea','OverallQual','GrLivArea','GarageArea',\n",
    "             'MoSold','YrSold','TotRmsAbvGrd']\n",
    "    \n",
    "    # select relevant features\n",
    "    X1 = X1[vars1]\n",
    "    \n",
    "    # scale data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(X1)\n",
    "    \n",
    "    return X1\n",
    "\n",
    "# perform preprocessing\n",
    "X_train1 = preprocess1(X_train)\n",
    "\n",
    "# transform test set so we get Apples to Apples comparison\n",
    "X_test1 = preprocess1(X_test) \n",
    "\n",
    "# define model: specify a number for random_state to ensure we get the same results each run\n",
    "# (decision trees incorporate randomness)\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# fit model\n",
    "dt_model.fit(X_train1, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "30e51ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "$RMSE = \\$41317.14$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a common error metric used in machine learning\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# for math computations\n",
    "import numpy as np \n",
    "\n",
    "# for fancy formatting w/ Latex\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "# make predictions\n",
    "predictions1 = dt_model.predict(X_test1)\n",
    "rmse1 = round(np.sqrt(mean_squared_error(predictions1,y_test)),2)\n",
    "\n",
    "# print rmse\n",
    "display(Markdown(rf\"\"\"$RMSE = \\${rmse1}$\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771006f4",
   "metadata": {},
   "source": [
    "#### Interpreting RMSE\n",
    "RMSE represents the average difference between your models' predictions and the actual values. Our model was on average, incorrect by $\\$41,317.14$. This is huge, so obviously our model is pretty bad, and we can't help our cousin, but we can apply a different strategy and see if that works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f271b",
   "metadata": {},
   "source": [
    "#### 2. Statistical methods\n",
    "Now we'll explore some statistical techniques for feature selection.\n",
    "\n",
    "One popular statistical method is applying a variance threshold. This method looks at our feature-space $X$ and will remove variables with low (or zero) variance. The intuition behind this technique is that features with no/low variance will always provide the same information no matter what the actual outcome is, thus it provides us with no information.\n",
    "\n",
    "Hence, we will remove such variables from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "98cbb498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 columns were selected:\n",
      "\n",
      "['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "def preprocess2(X):\n",
    "    \"\"\"\n",
    "    Preprocess our data by selecting the features\n",
    "    outlined above and then standardising the feature-space\n",
    "    \n",
    "    Returns: new, standardised feature-space and our original\n",
    "    target vector\n",
    "    \"\"\"\n",
    "    X1 = X.copy()\n",
    "    \n",
    "    # reduce subset to numeric data\n",
    "    X1 = X1[numeric_cols]\n",
    "    \n",
    "    # fill missing values\n",
    "    imputer = SimpleImputer(strategy = 'median')\n",
    "    X1 = imputer.fit_transform(X1)\n",
    "    \n",
    "    # scale data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(X1)\n",
    "    \n",
    "    # select features that have > 0 variance\n",
    "    selector = VarianceThreshold()\n",
    "    selector.fit(X1)\n",
    "    \n",
    "    col_names = [numeric_cols[i] for i in range(len(numeric_cols)) if \n",
    "                 selector.get_support()[i]]\n",
    "    \n",
    "    X1 = selector.transform(X1)\n",
    "    \n",
    "    return X1, col_names\n",
    "\n",
    "# perform preprocessing\n",
    "X_train2, col_names = preprocess2(X_train)\n",
    "\n",
    "# which columns were selected?\n",
    "print(f\"{len(col_names)} columns were selected:\\n\\n{col_names}\")\n",
    "\n",
    "# transform test set so we get Apples to Apples comparison\n",
    "X_test2 = X_test[col_names]\n",
    "\n",
    "# impute missing values\n",
    "imputer = SimpleImputer(strategy = 'median')\n",
    "X_test2 = imputer.fit_transform(X_test2)\n",
    "\n",
    "# define model. specify a number for random_state to ensure same results each run\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# fit model\n",
    "dt_model.fit(X_train2, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "240e5d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "$RMSE = \\$38256.19$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions2 = dt_model.predict(X_test2)\n",
    "rmse2 = round(np.sqrt(mean_squared_error(predictions2,y_test)),2)\n",
    "\n",
    "# print rmse\n",
    "display(Markdown(rf\"\"\"$RMSE = \\${rmse2}$\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af46fb43",
   "metadata": {},
   "source": [
    "Our predictions improved by roughly $\\$3000$ on average! This tells us that using all of the data is better than our intuition. We can use this information to update our intuition and consulting database...but still, a terrible outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9763dee3",
   "metadata": {},
   "source": [
    "#### 3. Applying Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8e48a5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 columns were selected:\n",
      "\n",
      "['OverallQual', 'BsmtFinSF1', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "def preprocess3(X):\n",
    "    \"\"\"\n",
    "    Preprocess our data by selecting the features\n",
    "    outlined above and then standardising the feature-space\n",
    "    \n",
    "    Returns: new, standardised feature-space and our original\n",
    "    target vector\n",
    "    \"\"\"\n",
    "    X1 = X.copy()\n",
    "    \n",
    "    # reduce subset to numeric data\n",
    "    X1 = X1[numeric_cols]\n",
    "    \n",
    "    # fill missing values\n",
    "    imputer = SimpleImputer(strategy = 'median')\n",
    "    X1 = imputer.fit_transform(X1)\n",
    "    \n",
    "    # scale data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(X1)\n",
    "    \n",
    "    # select features\n",
    "    # create & fit feature selection model\n",
    "    rf = RandomForestRegressor(n_estimators = 500,random_state = 42,\n",
    "                               max_depth = 10, n_jobs = -1,\n",
    "                               criterion='mse')\n",
    "    \n",
    "    feature_select_mod = SelectFromModel(estimator = rf).fit(X1,y_train)\n",
    "    \n",
    "    # get names of good columns\n",
    "    col_names = [numeric_cols[i] for i in range(len(numeric_cols)) if \n",
    "                 feature_select_mod.get_support()[i]]\n",
    "    \n",
    "    X2 = feature_select_mod.transform(X1)\n",
    "    \n",
    "    return X2, col_names\n",
    "\n",
    "# perform preprocessing\n",
    "X_train3, col_names = preprocess3(X_train)\n",
    "\n",
    "# which columns were selected?\n",
    "print(f\"{len(col_names)} columns were selected:\\n\\n{col_names}\")\n",
    "\n",
    "# transform test set so we get Apples to Apples comparison\n",
    "X_test3 = X_test[col_names]\n",
    "\n",
    "# impute missing values\n",
    "imputer = SimpleImputer(strategy = 'median')\n",
    "X_test3 = imputer.fit_transform(X_test3)\n",
    "\n",
    "# define model. specify a number for random_state to ensure same results each run\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# fit model\n",
    "dt_model.fit(X_train3, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7efd447a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "$RMSE = \\$37322.98$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions3 = dt_model.predict(X_test3)\n",
    "rmse3 = round(np.sqrt(mean_squared_error(predictions3,y_test)),2)\n",
    "\n",
    "# print rmse\n",
    "display(Markdown(rf\"\"\"$RMSE = \\${rmse3}$\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e3a428",
   "metadata": {},
   "source": [
    "Ok, wow, we did even better with only 6 variables! This demonstrates the power of feature selection, which can improve our ability to predict outcomes AND give us insight into variables that are the most important for predicting our outcomes. \n",
    "\n",
    "But why is the RMSE still so high? We can't get rich with our cousin if we don't reduce this value! We'll address this issue by considering models other than decision trees. The reason is: Decision trees are actually very bad IRL!!!\n",
    "\n",
    "To quote ***The Elements of Statistical Learning***: \n",
    ">\"Trees have one aspect that prevents them from being the ideal tool for predictive learning, namely **inaccuracy**.\" \n",
    "\n",
    "In other words, a decision tree is way too rigid to generalise!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa8c9fe",
   "metadata": {},
   "source": [
    "## Testing other ML models\n",
    "In this section, we're going to learn more about using the sci-kit learn library to create other models (like we did with our decision tree. When coding, this library is written as `sklearn`. Scikit-learn is easily the most popular library for modeling the types of data typically stored in our `pandas` DataFrames.\n",
    "\n",
    "The steps to building and using a model are:\n",
    "- Define: What type of model will it be? A tree? A linear model? Some other type of model? How will the parameters of the model be specified?\n",
    "- Fit: Apply the model to our data to capture patterns\n",
    "- Predict: Just what it sounds like\n",
    "- Evaluate: Determine how accurate the model's predictions are on a general data set\n",
    "\n",
    "Below, I'll show you a few examples of how to apply different machine learning models to our data. We're going to use `X_train2`, `X_train3`, and `X_test` to train our models and evaluate their predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "49737def",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (2286929518.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/p7/fqctjnc51g74qzr_wd6q7mpc0000gn/T/ipykernel_12335/2286929518.py\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    hidden_layer_sizes(64)),\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "# import models from relevant libraries\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def test_models(X_train,X_test,y_train,y_test):\n",
    "    \"\"\"\n",
    "    A function that will help us test a variety of specified models\n",
    "    \"\"\"\n",
    "\n",
    "    # dictionary of models we want to test\n",
    "    models = {'Neural Network':MLPRegressor(random_state=42, max_iter=500,\n",
    "                                           hidden_layer_sizes(64)),\n",
    "              'Linear Regression':LinearRegression(),\n",
    "              'Elastic Net':ElasticNet(),\n",
    "              'Ridge Regression':Ridge(),\n",
    "              'Lasso Regression':Lasso(),\n",
    "              'Support Vector Machine':SVR(),\n",
    "              'Random Forest':RandomForestRegressor(n_estimators = 500,random_state = 42,\n",
    "                                                    max_depth = 10, n_jobs = -1,\n",
    "                                                    #convention to use 20 to smooth splits\n",
    "                                                    criterion='mse',min_samples_split=20), \n",
    "              'Ada Boost':AdaBoostRegressor(n_estimators = 500,\n",
    "                                            random_state = 42,loss = 'square'),\n",
    "              'Gradient Boosting':GradientBoostingRegressor(n_estimators = 500,random_state = 42,\n",
    "                                                            loss = 'huber',criterion = 'mse',\n",
    "                                                            min_samples_split=20)}\n",
    "\n",
    "    # print RMSE of each model\n",
    "    for key in models.keys():\n",
    "        \n",
    "        # specify our model\n",
    "        mod = models[key]\n",
    "        \n",
    "        # fit our model\n",
    "        mod.fit(X_train, y_train)\n",
    "        \n",
    "        # generate predictions on unseen data\n",
    "        predictions = mod.predict(X_test)\n",
    "        \n",
    "        # compute our error metric, the RMSE\n",
    "        rmse = round(np.sqrt(mean_squared_error(predictions,y_test)),2)\n",
    "        mae = round(mean_absolute_error(predictions,y_test),2)\n",
    "        # display our error\n",
    "        display(Markdown(rf\"\"\"{key} $RMSE = \\${rmse}$\"\"\"))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e258a",
   "metadata": {},
   "source": [
    "You'll notice that some of the models I've specified have parameters explicitly written out. For instance, in the random forest model, you'll see that I've written `n_estimators = 500`. This specification is not necessary but can help in building more accurate models.  \n",
    "\n",
    "Also, although models like `ElasticNet` do not have parameters specified in their parentheses, they can accept parameter specifications. It's just that they are not necessary. To find the parameters that each model can accept parameters, you can look into their respective docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d800f69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Neural Network $RMSE = \\$48020.22$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Linear Regression $RMSE = \\$39154.13$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Elastic Net $RMSE = \\$39640.28$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Ridge Regression $RMSE = \\$39152.29$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Lasso Regression $RMSE = \\$39154.11$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Support Vector Machine $RMSE = \\$88595.36$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Random Forest $RMSE = \\$32342.56$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Ada Boost $RMSE = \\$40931.74$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Gradient Boosting $RMSE = \\$35074.78$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# run our function\n",
    "# using ML selected features\n",
    "test_models(X_train3,X_test3,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0525a690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Neural Network $RMSE = \\$42017.48$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Linear Regression $RMSE = \\$36846.58$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Elastic Net $RMSE = \\$37284.35$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Ridge Regression $RMSE = \\$36845.81$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Lasso Regression $RMSE = \\$36846.3$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Support Vector Machine $RMSE = \\$88652.4$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Random Forest $RMSE = \\$31159.27$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Ada Boost $RMSE = \\$38200.6$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Gradient Boosting $RMSE = \\$28526.51$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# using all numeric features\n",
    "test_models(X_train2,X_test2,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "07c86364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Neural Network $RMSE = \\$54207.74$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Linear Regression $RMSE = \\$42186.04$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Elastic Net $RMSE = \\$43306.24$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Ridge Regression $RMSE = \\$42186.43$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Lasso Regression $RMSE = \\$42186.05$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Support Vector Machine $RMSE = \\$88651.94$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Random Forest $RMSE = \\$36223.23$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Ada Boost $RMSE = \\$37344.57$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Gradient Boosting $RMSE = \\$34868.89$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# using features recommended by our consulting database/intuition\n",
    "test_models(X_train1,X_test1,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c90eca8",
   "metadata": {},
   "source": [
    "What you'll notice is that our random forest model works the best in all cases! You'll also see that using all of the numeric features produces the best performance. However, even if that is the case, you may prefer to only use the subset of ML selected features because of the principle of parsimony -- i.e. simpler models are better. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
